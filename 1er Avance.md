ğŸ›’ Product Data Pipeline
Este proyecto implementa un pipeline de procesamiento, limpieza y anÃ¡lisis de datos de productos, con foco en sus caracterÃ­sticas comerciales y precios en USD. EstÃ¡ diseÃ±ado para escalar sobre una arquitectura modular y basada en capas, utilizando Airflow y almacenamiento en la nube (AWS S3).

ğŸ“¦ Objetivo
Procesar datos de productos, limpiarlos, transformarlos y almacenarlos en capas organizadas (Bronze, Silver y potencialmente Gold) para su anÃ¡lisis posterior. Este pipeline permite identificar insights como productos mÃ¡s rentables, disponibilidad por categorÃ­a o comportamiento de precios.

ğŸ”— Fuentes de Datos
Tipo: Archivos CSV estructurados.

Ejemplo de campos:

Name, Description, Brand, Category

Price (USD), Currency, Stock, EAN

Color, Size, Availability, Internal ID

âš™ï¸ Estructura del Pipeline
ğŸ”¹ Ingesta
Orquestada por Airflow.

Un sensor diariamente chequea un bucket S3 para ver cuando se sube un nuevo archivo CSV y JSON.


Los archivos es copiado a la capa Bronze en S3.

ğŸ”¹ TransformaciÃ³n
Aplicada desde DAGs de Airflow.

ConversiÃ³n a Parquet, compresiÃ³n con Snappy y particionamiento por fecha.

Se eliminan inconsistencias, se validan tipos de datos y se estandarizan campos.

ğŸ”¹ Carga
Los datos refinados se cargan a la capa Silver en S3.

Futuramente se integrarÃ¡n con un data mart o data warehouse para anÃ¡lisis BI.

ğŸ—‚ï¸ Almacenamiento por Capas (Data Lake)
ğŸ¥‰ Bronze Layer
Origen: CSV, JSON o Parquet.

Destino: Archivos Parquet.

CompresiÃ³n: Snappy.

Particionamiento: AÃ±o > Mes > DÃ­a (basado en fecha de procesamiento).

TransformaciÃ³n: Solo cambio de formato y organizaciÃ³n.

RetenciÃ³n: Alta (3 aÃ±os). ConservaciÃ³n de data cruda e histÃ³rica.

ğŸ¥ˆ Silver Layer
Origen: Archivos Parquet provenientes de la capa Bronze.

Transformaciones:

ConversiÃ³n de precios a dÃ³lares si fuera necesario.

NormalizaciÃ³n de disponibilidad (backorder, pre_order, etc.).

Limpieza de descripciones, verificaciÃ³n de EANs.

CompresiÃ³n: Snappy.

Particionamiento: AÃ±o > Mes > DÃ­a.

RetenciÃ³n: Media (1.5 aÃ±os).

ğŸ¥‡ Gold Layer (No implementada en esta versiÃ³n)
Se reservÃ³ espacio para futuras mÃ©tricas o reportes.

Debido al bajo nivel de transformaciÃ³n requerido, no se implementÃ³ en esta entrega.

RetenciÃ³n estimada: Baja (0.5 aÃ±os).

ğŸ—ºï¸ Diagrama de Arquitectura
![Arquitectura del Pipeline](images/architecture.png)



â“ Preguntas de Negocio
âœ… 1. Â¿QuÃ© categorÃ­as de productos tienen mayor valor promedio por unidad?
ğŸ¯ Objetivo: Identificar oportunidades de margen alto y diseÃ±ar estrategias de precios.

ğŸ“Š Datos requeridos: Category, Price, Currency, Stock

ğŸ“ˆ Ejemplo de mÃ©trica:

```sql
Precio promedio por categorÃ­a = AVG(Price) GROUP BY Category
```
âœ… 2. Â¿QuÃ© disponibilidad de stock tienen los productos con precio mayor a $500 USD?
ğŸ¯ Objetivo: Evaluar la capacidad de entrega de productos premium.

ğŸ“Š Datos requeridos: Price, Availability, Stock

ğŸ§® Ejemplo de lÃ³gica:

```sql
SELECT Name, Price, Availability, Stock
FROM products
WHERE Price > 500
```
ğŸ³ Uso con Docker
El pipeline puede ejecutarse usando Docker con variables de entorno para las credenciales de AWS:

```bash
docker run \
  -e ACCESS_KEY=YOUR_ACCESS_KEY \
  -e SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY \
  henry-2do-pi

```
ğŸ” Permisos y Seguridad
Se otorgaron permisos IAM a un usuario externo de AWS para que pueda orquestar Airflow y acceder a los buckets.

El acceso estÃ¡ limitado a las acciones necesarias para la ingesta, transformaciÃ³n y carga de datos.